{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of a experience \n",
    "\n",
    "To calculate confusion matrixes and ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "\n",
    "# set dpi to 300\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "\n",
    "# create images folder\n",
    "image_path = os.path.join('..', '..', 'images')\n",
    "if not os.path.exists(image_path):\n",
    "    os.makedirs(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('logs', 'exp00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(os.path.join(path, 'models_mean_results.csv'))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_path = os.path.join(path, 'confusion-matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrices = {}\n",
    "for model in results.model:\n",
    "    cms = []\n",
    "    for fold in range(3):\n",
    "        aux = f'model_{model}_fold_{fold}_cm.csv'\n",
    "        cm = np.genfromtxt(os.path.join(cm_path, aux), delimiter=',')\n",
    "        cms.append(cm)\n",
    "    confusion_matrices[model] = cms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, cms in confusion_matrices.items():\n",
    "    aux_cm = np.array(cms).mean(axis=0)\n",
    "    group_counts = ['{0:0.0f}'.format(value) for value in\n",
    "                aux_cm.flatten()]\n",
    "    percentages_cm = (aux_cm.T / aux_cm.sum(axis=1)).T\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in\n",
    "                     percentages_cm.flatten()]\n",
    "    labels = [f'{v1}\\n({v2})' for v1, v2 in\n",
    "          zip(group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.set(font_scale=2.5)\n",
    "    sns.heatmap(aux_cm, annot=labels, fmt='', cmap='Blues')\n",
    "    plt.xlabel('Predicted Label', fontdict=dict(size=25))\n",
    "    plt.ylabel('True Label', fontdict=dict(size=25))\n",
    "    plt.savefig(os.path.join(image_path, f'average_cm_{model}_regression.png'), transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curves\n",
    "\n",
    "https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_path = os.path.join(path, 'predictions')\n",
    "preds_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_values = {}\n",
    "for model in results.model:\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    tpr_rates = []\n",
    "    roc_scores = []\n",
    "    for fold in range(1, 3+1):\n",
    "        aux_path = os.path.join(preds_path, f'{model}_fold{fold}_predictions.csv')\n",
    "        preds_df = pd.read_csv(aux_path)\n",
    "        roc_score = metrics.roc_auc_score(preds_df.y_test, preds_df.y_pred_proba)\n",
    "        roc_scores.append(roc_score)\n",
    "        fpr_proba, tpr_proba, threshold_proba = metrics.roc_curve(preds_df.y_test, preds_df.y_pred_proba)\n",
    "        interp_tpr = np.interp(mean_fpr, fpr_proba, tpr_proba)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tpr_rates.append(interp_tpr)\n",
    "    mean_tpr = np.mean(tpr_rates, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    preds_values[model] = {\n",
    "        'fpr' : mean_fpr,\n",
    "        'tpr' : mean_tpr,\n",
    "        'mean' : np.mean(roc_scores),\n",
    "        'std' : np.std(roc_scores)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "plt.figure(figsize=(15, 15))\n",
    "# set white grid\n",
    "sns.set_style(\"whitegrid\")\n",
    "for model, result in preds_values.items():\n",
    "    lw = 3\n",
    "    model_name = model.split(\"_\")[0]\n",
    "    plt.plot(\n",
    "        result['fpr'], \n",
    "        result['tpr'], \n",
    "        label=f'Mean ROC (AUC= {result[\"mean\"].round(3)} $\\pm$ {result[\"std\"].round(3)}) - {model_name}',\n",
    "        lw=lw)\n",
    "plt.legend(fontsize=\"14\")\n",
    "plt.plot([0, 1], [0, 1], linewidth=2, linestyle='dashed', color = 'g', label='Random Classifier')\n",
    "    \n",
    "plt.xlabel('False Positive Rate', fontdict=dict(size=20))\n",
    "plt.ylabel('True Positive Rate', fontdict=dict(size=20))\n",
    "plt.savefig(os.path.join(image_path, 'roc_curve.png'), transparent=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
