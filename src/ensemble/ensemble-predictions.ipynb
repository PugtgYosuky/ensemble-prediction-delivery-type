{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = '{image_model}'\n",
    "tabular_model = '{tabular_model}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = os.path.join('..', '..', 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_test, y_pred):\n",
    "    print('MCC', metrics.matthews_corrcoef(y_test, y_pred))\n",
    "    print('F1-score', metrics.f1_score(y_test, y_pred))\n",
    "    print('ROC-AUC', metrics.roc_auc_score(y_test, y_pred))\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "    metrics.ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_metrics(y_test, merge_cols, merge_func, roc_values):\n",
    "    results = pd.DataFrame()\n",
    "    cms = []\n",
    "    tpr_rates = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    for fold in range(1, 3+1):\n",
    "        df = pd.read_csv(f'predictions_{tabular_model}_{model}_fold_{fold}.csv')\n",
    "        df['Tabular_y_proba'] = df.Tabular_y_pred_proba\n",
    "        y_pred, y_proba = merge_func(df, merge_cols)\n",
    "        y_test = df.Tabular_y_test\n",
    "        tn, fp, fn, tp = metrics.confusion_matrix(y_test, y_pred).ravel()\n",
    "        cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "        cms.append(cm)\n",
    "        fpr_proba, tpr_proba, threshold_proba = metrics.roc_curve(y_test, y_proba)\n",
    "        # plt.plot(fpr_proba, tpr_proba, label=f'Fold {fold} - ROC AUC = {roc_score}', alpha=0.3,)\n",
    "        interp_tpr = np.interp(mean_fpr, fpr_proba, tpr_proba)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tpr_rates.append(interp_tpr)\n",
    "        res =  {\n",
    "          'models' : [merge_cols],\n",
    "          'ensemble_func' : merge_func.__name__.split('_')[1],\n",
    "          'fold' : fold,\n",
    "          'balanced_accuracy' : metrics.balanced_accuracy_score(y_test, y_pred),\n",
    "          'roc_auc_score' : [metrics.roc_auc_score(y_test, y_proba)],\n",
    "          'recall_weighted' : [metrics.recall_score(y_test, y_pred, average='weighted')], \n",
    "          'f1_weighted' : [metrics.f1_score(y_test, y_pred, average='weighted')], \n",
    "          'PPV_precision_weighted' : [metrics.precision_score(y_test, y_pred, average='weighted')], \n",
    "          'matthews_corrcoef' : [metrics.matthews_corrcoef(y_test, y_pred)],\n",
    "          'specificity' : [tn / (tn+fp)],\n",
    "          'NPV' : [tn / (tn + fn)],\n",
    "          'tp' : [tp],\n",
    "          'fp' : [fp],\n",
    "          'fn' : [fn],\n",
    "          'tn' : [tn]\n",
    "        }\n",
    "        results = pd.concat([results, pd.DataFrame(res)], ignore_index=True)\n",
    "    mean_tpr = np.mean(tpr_rates, axis=0)\n",
    "    roc_values[f'{merge_cols} - {merge_func.__name__.split(\"_\")[1].upper()}'] = {\n",
    "        'mean_tpr' : mean_tpr,\n",
    "        'mean_fpr' : mean_fpr,\n",
    "        'std_roc' : results.roc_auc_score.std().round(3),\n",
    "        'mean_roc' : results.roc_auc_score.mean().round(3),\n",
    "    }\n",
    "\n",
    "    # plot roc_curve\n",
    "    sns.set(font_scale=1.5)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.plot(\n",
    "        mean_fpr, \n",
    "        mean_tpr, \n",
    "        label=f'Mean ROC (AUC = {results.roc_auc_score.mean().round(3)} $\\pm$ {results.roc_auc_score.std().round(3)})',\n",
    "        lw=5\n",
    "    )\n",
    "    plt.legend(fontsize='14')\n",
    "    plt.plot([0, 1], [0, 1], linewidth=2, linestyle='dashed', color = 'g', label='Random Classifier')\n",
    "    \n",
    "    plt.xlabel('False Positive Rate', fontdict=dict(size=20))\n",
    "    plt.ylabel('True Positive Rate', fontdict=dict(size=20))\n",
    "    plt.title(f'ROC Curves', fontdict=dict(size=25))\n",
    "    plt.savefig(os.path.join(images_path, f'roc_curve_ensemble_{merge_cols}_{merge_func.__name__}.png'))\n",
    "    \n",
    "    # plot CM Matrix\n",
    "    mean_cm = np.array(cms).mean(axis=0)\n",
    "    group_counts = ['{0:0.0f}'.format(value) for value in\n",
    "                mean_cm.flatten()]\n",
    "    percentages_cm = (mean_cm.T / mean_cm.sum(axis=1)).T\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in\n",
    "                     percentages_cm.flatten()]\n",
    "    labels = [f'{v1}\\n({v2})' for v1, v2 in\n",
    "          zip(group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.set(font_scale=2.5)\n",
    "    sns.heatmap(\n",
    "        mean_cm, \n",
    "        annot=labels, \n",
    "        fmt='', \n",
    "        cmap='Blues', \n",
    "        xticklabels=['Vaginal Delivery', 'Cesarean Delivery'], \n",
    "        yticklabels=['Vaginal Delivery', 'Cesarean Delivery'])\n",
    "    \n",
    "    plt.xlabel('Predicted Label', fontdict=dict(size=25))\n",
    "    plt.ylabel('True Label', fontdict=dict(size=25))\n",
    "    plt.savefig(os.path.join(images_path, f'average_cm_{merge_cols}_{merge_func.__name__.split(\"_\")[1]}.png'), transparent=True)\n",
    "    return results, roc_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_values = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_max(df, merge_cols):\n",
    "    y_pred_cols = [f'{col}_y_pred' for col in merge_cols]\n",
    "    y_pred_proba_cols = [f'{col}_y_proba' for col in merge_cols]\n",
    "    return df[y_pred_cols].max(axis=1), df[y_pred_proba_cols].max(axis=1)\n",
    "\n",
    "def select_average(df, merge_cols):\n",
    "    cols = [f'{col}_y_proba' for col in merge_cols]\n",
    "    mean = df[cols].mean(axis=1)\n",
    "    preds = (mean > 0.5).astype(int)\n",
    "    return preds, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols = ['Femur']\n",
    "df, roc_values = get_cv_metrics('Tabular_y_test', merge_cols, select_max, roc_values)\n",
    "all_results = pd.concat([all_results, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols = ['Femur']\n",
    "df, roc_values = get_cv_metrics('Tabular_y_test', merge_cols, select_average, roc_values)\n",
    "all_results = pd.concat([all_results, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols = ['Tabular', 'Head', 'Femur', 'Abdomen']\n",
    "df, roc_values = get_cv_metrics('Tabular_y_test', merge_cols, select_average, roc_values)\n",
    "all_results = pd.concat([all_results, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols = ['Tabular', 'Head', 'Femur', 'Abdomen']\n",
    "df, roc_values = get_cv_metrics('Tabular_y_test', merge_cols, select_max, roc_values)\n",
    "all_results = pd.concat([all_results, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols = ['Femur', 'Abdomen', 'Head']\n",
    "df, roc_values = get_cv_metrics('Tabular_y_test', merge_cols, select_average, roc_values)\n",
    "all_results = pd.concat([all_results, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols = ['Femur', 'Abdomen', 'Head']\n",
    "df, roc_values = get_cv_metrics('Tabular_y_test', merge_cols, select_max, roc_values)\n",
    "all_results = pd.concat([all_results, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols = ['Tabular', 'Femur']\n",
    "df, roc_values = get_cv_metrics('Tabular_y_test', merge_cols, select_max, roc_values)\n",
    "all_results = pd.concat([all_results, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols = ['Tabular', 'Femur']\n",
    "df, roc_values = get_cv_metrics('Tabular_y_test', merge_cols, select_average, roc_values)\n",
    "all_results = pd.concat([all_results, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.models = all_results.models.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = all_results.pop('fold')\n",
    "mean = all_results.groupby(['models', 'ensemble_func']).mean()\n",
    "std = all_results.groupby(['models', 'ensemble_func']).std()\n",
    "mean = mean.add_prefix('mean_')\n",
    "std = std.add_prefix('std_')\n",
    "final_results = pd.concat([mean, std], axis=1)\n",
    "final_results = final_results.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results.to_csv(f'results_ensemble_{model}_{tabular_model}.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_values = dict(sorted(roc_values.items(), key=lambda x: x[1]['mean_roc'], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5)\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.set_style('whitegrid')\n",
    "for ensemble_model, values in roc_values.items():\n",
    "    if ensemble_model == \"['Tabular'] - MAX\":\n",
    "        ensemble_model = 'AdaBoostClassifier'\n",
    "    if ensemble_model == \"['Femur'] - MAX\":\n",
    "        ensemble_model = 'Inception Femur'\n",
    "    plt.plot(\n",
    "        values['mean_fpr'], \n",
    "        values['mean_tpr'], \n",
    "        label=f\"Mean ROC (AUC = {values['mean_roc']} $\\pm$ {values['std_roc']}) - {ensemble_model}\",\n",
    "        lw=3\n",
    "    )\n",
    "plt.plot([0, 1], [0, 1], linewidth=2, linestyle='dashed', color = 'g', label='Random Classifier')\n",
    "plt.legend(fontsize='14')\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontdict=dict(size=20))\n",
    "plt.ylabel('True Positive Rate', fontdict=dict(size=20))\n",
    "plt.title(f'ROC Curves', fontdict=dict(size=25))\n",
    "plt.savefig(os.path.join(images_path, f'roc_curve_ensemble_all.png'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
