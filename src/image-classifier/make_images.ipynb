{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of a experience \n",
    "\n",
    "To calculate confusion matrixes and ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '{model_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(f'logs-{model_name}') # INSERT HERE THE LOGS PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = os.path.join('..', '..', 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'Head' : {},\n",
    "    'Femur' : {},\n",
    "    'Abdomen' : {}\n",
    "}\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "for exp in os.listdir(path):\n",
    "    df = pd.read_csv(os.path.join(path, exp, 'test_prediction.csv'))\n",
    "    with open(os.path.join(path, exp, 'config.json')) as file:\n",
    "        config = json.load(file)\n",
    "    eco_type = config['dataset'].split('/')[1]\n",
    "    cm = metrics.confusion_matrix(df.y_test, df.y_pred)\n",
    "    results[eco_type]['cm'] = results[eco_type].get('cm', []) + [cm]\n",
    "\n",
    "    roc_score = metrics.roc_auc_score(df.y_test, df.y_proba)\n",
    "    fpr_proba, tpr_proba, threshold_proba = metrics.roc_curve(df.y_test, df.y_proba)\n",
    "    interp_tpr = np.interp(mean_fpr, fpr_proba, tpr_proba)\n",
    "    interp_tpr[0] = 0.0\n",
    "    results[eco_type]['tpr_rates'] = results[eco_type].get('tpr_rates', []) + [interp_tpr]\n",
    "    results[eco_type]['mean_fpr'] = mean_fpr\n",
    "    results[eco_type]['roc_auc'] = results[eco_type].get('roc_auc', []) + [roc_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eco_type, values in results.items():\n",
    "    # plot cm matrix\n",
    "    average_cm = np.array(values['cm']).mean(axis=0)\n",
    "    group_counts = ['{0:0.0f}'.format(value) for value in\n",
    "                average_cm.flatten()]\n",
    "    percentages_cm = (average_cm.T / average_cm.sum(axis=1)).T\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in\n",
    "                     percentages_cm.flatten()]\n",
    "    labels = [f'{v1}\\n({v2})' for v1, v2 in\n",
    "          zip(group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.set(font_scale=2.5)\n",
    "    sns.heatmap(average_cm, \n",
    "                annot=labels, \n",
    "                fmt='', \n",
    "                cmap='Blues',\n",
    "                xticklabels=['Vaginal Delivery', 'Cesarean Delivery'], \n",
    "                yticklabels=['Vaginal Delivery', 'Cesarean Delivery'])\n",
    "    plt.xlabel('Predicted Label', fontdict=dict(size=25))\n",
    "    plt.ylabel('True Label', fontdict=dict(size=25))\n",
    "    plt.savefig(os.path.join(images_path, f'average_cm_{model_name}_{eco_type}.png'), transparent=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "sns.set(font_scale=2)\n",
    "sns.set_style(\"whitegrid\")\n",
    "for eco_type, values in results.items():\n",
    "    mean_tpr = np.mean(np.array(values['tpr_rates']), axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    plt.plot(\n",
    "        values['mean_fpr'],\n",
    "        mean_tpr,\n",
    "        label=f'Mean ROC (AUC={np.mean(values[\"roc_auc\"]).round(3)} $\\pm$ {np.std(values[\"roc_auc\"]).round(3)}) - {eco_type.strip(\"_\")}',\n",
    "        lw=3\n",
    "        )\n",
    "plt.plot([0, 1], [0, 1], linewidth=2, linestyle='dashed', color = 'g', label='Random Classifier')\n",
    "plt.legend(fontsize=\"14\")\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontdict=dict(size=25))\n",
    "plt.ylabel('True Positive Rate', fontdict=dict(size=25))\n",
    "plt.savefig(os.path.join(images_path, f'roc_curves_image_classifiers_{model_name}.png'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
